# Dataset used: Microsoft Malware Classification Challenge (BIG 2015)
# Citation: http://arxiv.org/abs/1802.10135
# Alessandro Panconesi, Marian, Will Cukierski, WWW BIG - Cup Committee. (2015).
# Microsoft Malware Classification Challenge (BIG 2015). Kaggle. https://kaggle.com/competitions/malware-classification

# BIG 2015 9-Class Malware Classification
# MinMax Normalization, Conditional Tabular Generative Adversarial Network (CTGAN), Train/Test Split, Stacking

# Written in Anaconda Spyder 5.5.0 IDE using Python 3.9.18 64-bit on Windows 10

# Necessary Imports
import numpy as np
import pandas as pd
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2

# CTGAN Imports
from pmlb import fetch_data
from ydata_synthetic.synthesizers.regular import RegularSynthesizer
from ydata_synthetic.synthesizers import ModelParameters, TrainParameters

# Stacking Imports
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score, roc_curve, auc
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.feature_selection import SelectFromModel
from keras.models import load_model
import xgboost as xgb

#%%

""" Load Dataset """ # The dataset used was generated from feature extraction

# Microsoft Malware Classification Challenge, xx features, extracted from xxxxx .asm/.BYTES files, Multiclass Classification
dataset = pd.read_csv('malware_dataset.csv')

# 'index' and 'hash_value' are not useful for training purposes
dataset.drop('index', axis=1, inplace=True)
dataset.drop('hash_value', axis=1, inplace=True)

# For the purposes of focusing only on malware classification, every sample will be malicious, so 'label' is not needed
dataset.drop('label', axis=1, inplace=True)

#%%

""" Data Preprocessing """

# 'class' is already label encoded by default, so LabelEncoding is not needed

# Define the columns to LabelEncode
categorical_columns=[] # none

# Encode categorical columns using LabelEncoder
label_encoder = preprocessing.LabelEncoder()
for column in categorical_columns:
    dataset[column] = label_encoder.fit_transform(dataset[column])

# Define the columns to scale
columns_to_scale=['mov', 'call', 'push', 'pop', 'xor', 'sub', 'add', 'jmp', 'asm_file_size', 'asm_entropy', 'num_bytes',
                  'proc_count', '.text_count', '.data_count', '.bss_count', '.rodata_count', 'comment_count']

# Scale numerical columns using MinMax
scaler = MinMaxScaler()
for column in columns_to_scale:
    dataset[column] = scaler.fit_transform(dataset[[column]])

# Extract X and y from the preprocessed DataFrame
X = dataset.iloc[:, :-1].values # exclude 'class'
y = dataset['class'].values

#%%
""" CTGAN Synthetic Data Generation """

# Load data for synthetic data creation
data = dataset

num_cols = ['mov', 'call', 'push', 'pop', 'xor', 'sub', 'add', 'jmp', 'asm_file_size', 'asm_entropy', 'num_bytes', 
            'proc_count', '.text_present', '.data_present', '.bss_present', '.rodata_present', '.text_count',
            '.data_count', '.bss_count', '.rodata_count', 'comment_count']


cat_cols = ['class']

# Distribution of classes in dataset
data['class'].value_counts()

#%%
""" Define model and training parameters """

# Defining the training parameters
batch_size = 5000 #5000
epochs = 100 # originally 100
learning_rate = 2e-4 #2e-4
beta_1 = 0.5 #0.5
beta_2 = 0.9 #0.9

ctgan_args = ModelParameters(batch_size=batch_size,
                             lr=learning_rate,
                             betas=(beta_1, beta_2))

train_args = TrainParameters(epochs=epochs)

#%%
""" Create and Train the CTGAN """

print("Training In Progress...")

synth = RegularSynthesizer(modelname='ctgan', model_parameters=ctgan_args)
synth.fit(data=data, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols)

print("Training Complete!")
#%%
""" Generate new synthetic data"""

synth_data = synth.sample(1250) # originally 5,000
#print(synth_data)

#%%
""" OPTIONAL to Save Synth Dataset """

# Use this code if you want to save the synthetic data for future use or examination
synth_data.to_csv('synth_datax.csv', index=False)

#%%
""" OPTIONAl to Import Synthetic Data """

# Use this code if you already have already generated synthetic data
synth_data = pd.read_csv('synth_datax.csv') # Change as needed


#%%
""" Compare Real Samples to Synthetic """

print(synth_data['class'].value_counts())

#%%
""" Drop Unneeded Synthetic Samples (OPTIONAL)""" 

# There is already a large sample size for class '4' and '0', no need for additional synthetic data
# Drop rows with class '4' or '0'
#filtered_synth_data = synth_data[(synth_data['class'] != 4) & (synth_data['class'] != 0)]
filtered_synth_data = synth_data

#%%
""" Concatenation of Synthetic Data with Real Data """

# Assuming synth_data and KDDTrain are both pandas DataFrames
# If they are not, you can convert them to DataFrames using pd.DataFrame()

# Concatenate the synthetic samples to the original dataset
concatenated_data = pd.concat([dataset, filtered_synth_data], ignore_index=True)

# Distribution of classes in dataset after synthetic concatenation
print(concatenated_data['class'].value_counts())

# Extract X and y from the preprocessed DataFrame
X = concatenated_data.iloc[:, :-1].values # exclude 'class'
y = concatenated_data['class'].values

#%%
""" Train/Test Dataset Split """

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)


#%%
""" Stacking + Test Performance/Cross-Validation """

# Performance comparison classifiers as evaluated on 10-Fold cross validation + independent test (Stacking)

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score, roc_curve, auc
from keras.models import load_model
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import VotingClassifier
from xgboost import XGBClassifier

# Define a diverse set of base estimators
base_estimators = [
    ('xgb', XGBClassifier(random_state=0,colsample_bytree=0.8, gamma=0, learning_rate=0.2, max_depth=3, n_estimators=185, reg_alpha=0, reg_lambda=0, subsample=0.8,n_jobs=-1)),
    ('nb', GaussianNB(var_smoothing=1e-4)),
    ('ada', AdaBoostClassifier(n_estimators=225,random_state=0,learning_rate=0.125))
]

# Stacking Classifier
stacking_model = StackingClassifier(estimators=base_estimators, 
                                     final_estimator=RandomForestClassifier(max_depth=20, random_state=0, min_samples_split=3, n_estimators=100, n_jobs=-1))

# Train the model on the entire training set
stacking_model.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = stacking_model.predict(X_test)

# Perform 10-fold cross-validation on the entire dataset
cv_scores = cross_val_score(stacking_model, X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1)

# Print the cross-validation scores
print("Cross-Validation scores:\n", cv_scores)
print("\nMean CV Score:", cv_scores.mean())

# Calculate Matthews correlation coefficient (MCC)
print("\nMatthews correlation coefficient (MCC):", matthews_corrcoef(y_test, y_pred))

# Generate classification report
classification_rep = classification_report(y_test, y_pred, digits=4)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

#%%

""" Saving the Model for future use in Scoring Scripts and/or Mobile App """

# Serialize the trained model using pickle
with open('malwareClassification.pkl', 'wb') as f:
    pickle.dump(stacking_model, f)
    
#%%

""" Visualize Results """

def plot_confusion_matrix_heatmap_with_values(cm, classes, title, cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if cm.max() < 1 else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], fmt),
                     ha="center", va="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.xlabel('Predicted Class')
    plt.ylabel('True Class')

#class_names = ['Ramnit', 'Lollipop', 'Kelihos_ver3', 'Vundo', 'Simda', 'Tracur', 'Kelihos_ver1', 'Obfuscator.ACY', 'Gatak']
class_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9']

# Original Confusion Matrix
cm_original = confusion_matrix(y_test, y_pred)
plot_confusion_matrix_heatmap_with_values(cm_original, class_names, 'Malware Classification')

# Call plt.tight_layout() before saving
plt.tight_layout()

plt.savefig('Heatmap_400dpi.png', dpi=400)  # Saving with 400 dpi
